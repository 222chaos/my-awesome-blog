## 实现模型对话流式输出

### 现有代码分析
- 后端已实现 `stream_chat` 抽象方法和 DeepSeek 提供商的流式支持
- 前端有 `MessageBubble` 组件支持 `isStreaming` 状态
- 需要将前后端连接起来

### 实现计划

#### 1. 后端 API 端点 (`api/v1/endpoints/llm.py`)
- 修改 `/api/v1/llm/stream-chat` 端点
- 使用 `StreamingResponse` 返回 Server-Sent Events (SSE) 格式
- 使用 `llm_service.get_provider().stream_chat()` 获取流式数据

#### 2. 前端服务 (`services/llmService.ts`)
- 确保 `streamChat` 方法正确处理 SSE 流
- 使用 `TextDecoder` 解码数据
- 逐块返回内容

#### 3. 前端页面 (`app/ai/chat/page.tsx`)
- 使用 `streamChat` 方法
- 实时更新消息内容
- 显示流式打字效果

### SSE 响应格式
```
data: {"content": "你好", "finish_reason": null}

data: {"content": "，", "finish_reason": null}

data: {"content": "有什么", "finish_reason": null}

data: {"content": "可以帮", "finish_reason": "stop"}
```

### 实现效果
- AI 回复逐字显示
- 带有打字光标动画
- 流式传输完成后移除光标