"""
æµ‹è¯• LLM API è°ƒç”¨
"""
import asyncio
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.llm import get_llm_provider, ChatCompletionRequest, ChatMessage


async def test_deepseek():
    """æµ‹è¯• DeepSeek API è°ƒç”¨"""
    print("=" * 50)
    print("å¼€å§‹æµ‹è¯• DeepSeek API è°ƒç”¨...")
    print("=" * 50)

    try:
        provider = get_llm_provider('deepseek')

        if provider is None:
            print("âŒ å¤±è´¥: DeepSeek Provider åˆ›å»ºå¤±è´¥")
            print("   è¯·æ£€æŸ¥ DEEPSEEK_API_KEY æ˜¯å¦å·²æ­£ç¡®é…ç½®")
            return

        print(f"âœ… Provider åˆ›å»ºæˆåŠŸ")
        print(f"   Provider: {provider.get_provider_name()}")
        print(f"   Model: {provider.get_model_name()}")
        print()

        request = ChatCompletionRequest(
            messages=[
                ChatMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
            ],
            temperature=0.7,
            stream=False
        )

        print("ğŸ“¤ å‘é€è¯·æ±‚: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
        print()

        response = await provider.chat(request)

        print("ğŸ“¥ æ”¶åˆ°å“åº”:")
        print(f"   Role: {response.message.role}")
        print(f"   Content: {response.message.content}")
        print(f"   Model: {response.model}")

        if response.usage:
            print(f"   Usage:")
            print(f"     - Prompt tokens: {response.usage.prompt_tokens}")
            print(f"     - Completion tokens: {response.usage.completion_tokens}")
            print(f"     - Total tokens: {response.usage.total_tokens}")

        print()
        print("=" * 50)
        print("âœ… DeepSeek API è°ƒç”¨æµ‹è¯•æˆåŠŸï¼")
        print("=" * 50)

    except Exception as e:
        print()
        print("=" * 50)
        print(f"âŒ DeepSeek API è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼")
        print(f"   é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {e}")
        print("=" * 50)


async def test_stream():
    """æµ‹è¯•æµå¼è°ƒç”¨"""
    print()
    print("=" * 50)
    print("å¼€å§‹æµ‹è¯• DeepSeek æµå¼ API è°ƒç”¨...")
    print("=" * 50)

    try:
        provider = get_llm_provider('deepseek')

        if provider is None:
            print("âŒ å¤±è´¥: DeepSeek Provider åˆ›å»ºå¤±è´¥")
            return

        request = ChatCompletionRequest(
            messages=[
                ChatMessage(role="user", content="è¯·å†™ä¸€é¦–äº”è¨€ç»å¥ï¼Œå…³äºæ˜¥å¤©çš„è¯—")
            ],
            temperature=0.8,
            stream=True
        )

        print("ğŸ“¤ å‘é€è¯·æ±‚: è¯·å†™ä¸€é¦–äº”è¨€ç»å¥ï¼Œå…³äºæ˜¥å¤©çš„è¯—")
        print()
        print("ğŸ“¥ æµå¼å“åº”:")

        full_content = ""
        async for chunk in provider.stream_chat(request):
            if chunk.content:
                print(chunk.content, end="", flush=True)
                full_content += chunk.content
            if chunk.finish_reason:
                print()

        print()
        print()
        print("=" * 50)
        print("âœ… DeepSeek æµå¼ API è°ƒç”¨æµ‹è¯•æˆåŠŸï¼")
        print("=" * 50)

    except Exception as e:
        print()
        print("=" * 50)
        print(f"âŒ DeepSeek æµå¼ API è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼")
        print(f"   é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {e}")
        print("=" * 50)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    await test_deepseek()
    await test_stream()


if __name__ == "__main__":
    asyncio.run(main())
